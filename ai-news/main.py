import os
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate


# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 1. ì„¤ì • (API í‚¤ í•„ìš”)
OPEN_AI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

def summarize_news(topic):
    # 2. ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” (ìµœì‹  ê¸°ì‚¬ 3ê°œ ê²€ìƒ‰)
    search = TavilySearch(k=3)

    print(f"ğŸ” '{topic}' ê´€ë ¨ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...")
    try:
        search_results = search.invoke(topic)
    except Exception as e:
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    # 3. ê²€ìƒ‰ëœ ë‚´ìš© í•©ì¹˜ê¸° (TavilyëŠ” URLê³¼ í•¨ê»˜ ìš”ì•½ëœ ë‚´ìš©ë„ ì¼ë¶€ ë°˜í™˜í•¨)
    context_text = ""
    for item in search_results['results']:
        context_text += f"ì¶œì²˜: {item['url']}\në‚´ìš©: {item['content']}\n\n"

    # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    prompt = ChatPromptTemplate.from_template("""
    ì•„ë˜ ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{topic}'ì— ëŒ€í•œ ìµœì‹  ë™í–¥ì„ ìš”ì•½í•´ì¤˜.
    
    [ìš”êµ¬ì‚¬í•­]
    1. ê° ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ í†µí•©í•´ì„œ 3ë¬¸í•­ìœ¼ë¡œ ë‚˜ì—´í•˜ì—¬ ì„œìˆ í•  ê²ƒ.
    2. ì „ë¬¸ì ì¸ í†¤ì•¤ë§¤ë„ˆë¥¼ ìœ ì§€í•  ê²ƒ.
    3. ë§ˆì§€ë§‰ì— ì›ë³¸ ì¶œì²˜ ë§í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì œê³µí•  ê²ƒ.
    4. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ë‚˜ ì¥í™©í•œ ì„¤ëª…ì€ í”¼í•  ê²ƒ.
    5. ìµœì‹  ì •ë³´ì— ì§‘ì¤‘í•  ê²ƒ.
    
    [ê¸°ì‚¬ ë°ì´í„°]
    {context}
    """)
    
    # 5. ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
    print("ğŸ¤– AIê°€ ìš”ì•½ ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤...")
    chain = prompt | llm
    summary = chain.invoke({"topic": topic, "context": context_text})
    
    return summary.content

# ì‹¤í–‰
if __name__ == "__main__":
    user_input = input("ìš”ì•½í•  ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    result = summarize_news(user_input)
    print("\n" + "="*50)
    print(result)
    print("="*50)